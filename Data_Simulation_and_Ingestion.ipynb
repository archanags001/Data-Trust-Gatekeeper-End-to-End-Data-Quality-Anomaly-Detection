{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ef00956a-0bb8-4514-9500-74609c0263f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Data Trust Gatekeeper: Real-Time Anomaly Detection Solution :Part 1\n",
    "## PART 1 : Data Simulation and Ingestion\n",
    "**Objective:**\n",
    "1. Define project configurations and clean up previous runs.\n",
    "2. Generate a simulated, end-to-end customer acquisition dataset (Visitor -> Application -> Account -> Transaction).\n",
    "3. Inject synthetic TDQ, BDQ, and ML anomalies to test the detection pipeline.\n",
    "4. Write data across a 21-day period to the created Unity Catalog Volume path.\n",
    "  * **14 Days of Normal Historical Data**(Stable patterns,No anomalies, Used to train ML time-series models and baseline TDQ/BDQ thresholds.)\n",
    "  \n",
    "  * **7 Days of Live Data With Controlled Anomalies**\n",
    "  -  Each day contains 8 micro-batches (3-hour intervals), simulating:\n",
    "  -  Real-time ingestion\n",
    "  -  Event stream-style arrival\n",
    "  -  Mixed normal + anomalous segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e18d90f-b2b6-4397-b31d-7a6e349999c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faker\n  Downloading faker-38.2.0-py3-none-any.whl.metadata (16 kB)\nRequirement already satisfied: tzdata in /databricks/python3/lib/python3.12/site-packages (from faker) (2024.1)\nDownloading faker-38.2.0-py3-none-any.whl (2.0 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/2.0 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.0/2.0 MB\u001B[0m \u001B[31m22.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hInstalling collected packages: faker\nSuccessfully installed faker-38.2.0\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install faker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7740a116-b86b-420c-81c3-751bc4fec389",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%restart_python "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f38d9933-350a-4255-8760-0e25b047e1e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.databricks.empty-table+json": {
       "directive_name": "NoDirective"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "CREATE VOLUME IF NOT EXISTS workspace.default.datagov_project_data;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4fcfaa2-2b36-4213-b650-0f618746bcf6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from faker import Faker\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import current_timestamp, date_format\n",
    "\n",
    "# --- 1. CONFIGURATION AND INITIALIZATION ---\n",
    "\n",
    "# Initialize Spark Session and Faker for data generation\n",
    "spark = SparkSession.builder.appName(\"DataTrustGatekeeper_Setup\").getOrCreate()\n",
    "fake = Faker()\n",
    "np.random.seed(42) \n",
    "\n",
    "# PRODUCTION CONFIG: Define the Root Path for data storage \n",
    "CATALOG_NAME = \"workspace\" \n",
    "SCHEMA_NAME = \"default\" \n",
    "VOLUME_NAME = \"datagov_project_data\"\n",
    "\n",
    "# Full path to the project's data storage location\n",
    "BASE_PATH_UC = f\"/Volumes/{CATALOG_NAME}/{SCHEMA_NAME}/{VOLUME_NAME}/data_trust_gatekeeper_v0/\"\n",
    "LANDING_PATHS = {\n",
    "    \"visitor\": f\"{BASE_PATH_UC}landing/visitor_events/\",\n",
    "    \"app\": f\"{BASE_PATH_UC}landing/applications/\",\n",
    "    \"account\": f\"{BASE_PATH_UC}landing/accounts/\",\n",
    "    \"txn\": f\"{BASE_PATH_UC}landing/transactions/\"\n",
    "}\n",
    "\n",
    "CONTROLLED_GEOS = [\"USA\", \"UK\", \"IND\", \"CA\"] \n",
    "VALID_EVENT_TYPES = [\"landing\", \"scroll\", \"apply_click\", \"view_pricing\"]\n",
    "VALID_APP_STATUSES = [\"submitted\", \"approved\", \"rejected\"]\n",
    "\n",
    "\n",
    "# --- 2. ENVIRONMENT SETUP & CLEANUP ---\n",
    "\n",
    "print(f\"1. Cleaning up previous environment at UC Volume path: {BASE_PATH_UC}\")\n",
    "\n",
    "# 2.1 Clean up the entire base path recursively\n",
    "dbutils.fs.rm(BASE_PATH_UC, recurse=True)\n",
    "time.sleep(1) \n",
    "\n",
    "# 2.2 Create the new directory structure for landing zones\n",
    "for p in LANDING_PATHS.values():\n",
    "    dbutils.fs.mkdirs(p)\n",
    "    print(f\"   -> Created Volume path: {p}\")\n",
    "\n",
    "\n",
    "# --- 3. HELPER FUNCTIONS FOR SIMULATION ---\n",
    "\n",
    "def generate_batch(start_time: datetime, num_records: int, anomaly_type: str = None):\n",
    "    \"\"\"\n",
    "    Generates a synchronized batch of data simulating the customer acquisition funnel.\n",
    "    It intentionally injects specific anomalies based on the 'anomaly_type' flag.\n",
    "    \"\"\"\n",
    "    \n",
    "    visitors, applications, accounts, transactions = [], [], [], []\n",
    "    \n",
    "    # Base Conversion Rates\n",
    "    application_prob = 0.20 \n",
    "    approval_prob = 0.60    \n",
    "    txn_prob = 0.10         \n",
    "    \n",
    "    # --- ANOMALY INJECTION LOGIC (Testing TDQ, BDQ, and ML detection) ---\n",
    "    if anomaly_type == 'conversion_drop': \n",
    "        application_prob = 0.05 \n",
    "    if anomaly_type == 'traffic_spike': \n",
    "        num_records = int(num_records * 2.5) \n",
    "\n",
    "    # FRESHNESS ANOMALY INJECTION\n",
    "    time_offset_seconds = 0\n",
    "    if anomaly_type == 'stale_data': \n",
    "        # Force timestamps to be 12 hours in the past relative to the batch time\n",
    "        time_offset_seconds = -12 * 3600\n",
    "        print(f\"      -> Injecting STALE DATA (Offset: {time_offset_seconds}s)\") \n",
    "        \n",
    "\n",
    "    \n",
    "    # --- Generate Visitor Events ---\n",
    "    for i in range(num_records):\n",
    "        vis_id = fake.uuid4()\n",
    "        event_time = (start_time + timedelta(seconds=np.random.randint(1, 100)) + \n",
    "                      timedelta(seconds=time_offset_seconds))\n",
    "        \n",
    "        campaign = fake.word().upper() + str(np.random.randint(100, 999))\n",
    "        \n",
    "        # Inject Null Campaign IDs (completeness/null rate failure)\n",
    "        if anomaly_type == 'null_burst' and np.random.rand() < 0.25:\n",
    "            campaign = None \n",
    "        \n",
    "        duration = np.random.normal(loc=30, scale=10)\n",
    "        \n",
    "        # Low session duration indicating bot/poor traffic quality\n",
    "        if anomaly_type == 'traffic_spike' and np.random.rand() < 0.8:\n",
    "            duration = np.random.uniform(low=0.1, high=1.0) \n",
    "\n",
    "        visitors.append({\n",
    "            \"visitor_id\": vis_id,\n",
    "            \"event_type\": np.random.choice(VALID_EVENT_TYPES),\n",
    "            \"campaign_id\": campaign,\n",
    "            \"device_type\": np.random.choice([\"mobile\", \"desktop\", \"tablet\"]),\n",
    "            \"geo\": np.random.choice(CONTROLLED_GEOS), \n",
    "            \"timestamp\": event_time,\n",
    "            \"session_duration\": max(0.1, duration)\n",
    "        })\n",
    "\n",
    "        # --- Generate Applications ---\n",
    "        if np.random.rand() < application_prob:\n",
    "            app_id = fake.uuid4()\n",
    "            age = np.random.randint(22, 65)\n",
    "            income = np.random.randint(40000, 150000)\n",
    "            app_time = event_time + timedelta(minutes=np.random.randint(1, 15))\n",
    "            \n",
    "            # Inject under-age applicant (Business Rule Violation: Age < 18)\n",
    "            if anomaly_type == 'age_error' and np.random.rand() < 0.15:\n",
    "                age = np.random.randint(10, 17) \n",
    "            \n",
    "            is_approved = np.random.rand() < approval_prob and age >= 18\n",
    "            \n",
    "            if is_approved:\n",
    "                app_status = \"approved\"\n",
    "            elif age < 18:\n",
    "                app_status = \"rejected - under age\" \n",
    "            else:\n",
    "                app_status = np.random.choice([\"rejected\", \"submitted\"], p=[0.9, 0.1])\n",
    "            \n",
    "            req_limit = int(np.floor(income / 10))\n",
    "\n",
    "            # High requested limit relative to income\n",
    "            if anomaly_type == 'limit_spike' and np.random.rand() < 0.1:\n",
    "                req_limit = int(income * np.random.uniform(0.6, 0.9)) \n",
    "            \n",
    "            applications.append({\n",
    "                \"app_id\": app_id,\n",
    "                \"visitor_id\": vis_id,\n",
    "                \"applicant_age\": age,\n",
    "                \"annual_income\": float(income),\n",
    "                \"employment_status\": np.random.choice([\"salaried\", \"self-employed\", \"unemployed\"]),\n",
    "                \"req_credit_limit\": req_limit,\n",
    "                \"timestamp\": app_time,\n",
    "                \"application_status\": app_status \n",
    "            })\n",
    "            \n",
    "            # --- Generate Accounts (ONLY IF approved) ---\n",
    "            if app_status == 'approved':\n",
    "                account_id = fake.uuid4()\n",
    "                open_time = app_time + timedelta(hours=np.random.randint(2, 24))\n",
    "                limit = int(np.floor(income / 10) * np.random.uniform(0.8, 1.2))\n",
    "                \n",
    "                accounts.append({\n",
    "                    \"account_id\": account_id,\n",
    "                    \"app_id\": app_id,\n",
    "                    \"approved_limit\": limit,\n",
    "                    \"interest_rate\": np.random.uniform(14.99, 29.99),\n",
    "                    \"account_open_timestamp\": open_time,\n",
    "                    \"account_status\": \"active\" \n",
    "                })\n",
    "                \n",
    "                # --- Generate Transactions (Usage) ---\n",
    "                if np.random.rand() < txn_prob:\n",
    "                    num_txns = np.random.randint(1, 5)\n",
    "                    for k in range(num_txns):\n",
    "                        txn_time = open_time + timedelta(days=np.random.randint(1, 5), minutes=np.random.randint(0, 1440))\n",
    "                        amount = np.random.uniform(5.0, 500.0)\n",
    "                        \n",
    "                        # Inject large fraud transaction (Outlier Detection)\n",
    "                        if anomaly_type == 'fraud_spike' and np.random.rand() < 0.05:\n",
    "                            amount = np.random.uniform(limit * 0.7, limit * 0.9) \n",
    "                        \n",
    "                        transactions.append({\n",
    "                            \"txn_id\": fake.uuid4(),\n",
    "                            \"account_id\": account_id,\n",
    "                            \"amount\": round(amount, 2),\n",
    "                            \"merchant_cat_code\": np.random.choice([\"5411\", \"5812\", \"5999\", \"7999\", \"5541\"]),\n",
    "                            \"location\": fake.country(),\n",
    "                            \"state\": fake.state_abbr(),\n",
    "                            \"merchant_city\": fake.city(),\n",
    "                            \"timestamp\": txn_time\n",
    "                        })\n",
    "\n",
    "    return (pd.DataFrame(visitors), \n",
    "            pd.DataFrame(applications), \n",
    "            pd.DataFrame(accounts), \n",
    "            pd.DataFrame(transactions))\n",
    "\n",
    "def write_data_to_landing_zone(pdf: pd.DataFrame, path: str, suffix: str, timestamp_col: str):\n",
    "    \"\"\"\n",
    "    Converts a Pandas DF to Spark DF, adds a date column, and writes as Parquet \n",
    "    to the designated landing zone, partitioned by date.\n",
    "    \"\"\"\n",
    "    if pdf.empty:\n",
    "        return\n",
    "    \n",
    "    pdf['date'] = pd.to_datetime(pdf[timestamp_col]).dt.strftime('%Y-%m-%d')\n",
    "    \n",
    "    sdf = spark.createDataFrame(pdf)\n",
    "\n",
    "    full_path = f\"{path}{suffix}\"\n",
    "    (sdf.write\n",
    "        .mode(\"append\")\n",
    "        .partitionBy(\"date\") \n",
    "        .parquet(full_path)) \n",
    "    print(f\"   -> Wrote {len(pdf):,} records to {path}\")\n",
    "\n",
    "\n",
    "# --- 4. EXECUTION: HISTORICAL DATA GENERATION (14 DAYS) ---\n",
    "\n",
    "# onfiguration for 21-Day Run\n",
    "HISTORICAL_DAYS = 14\n",
    "ANOMALY_INJECTION_DAYS = 7\n",
    "TOTAL_DAYS = HISTORICAL_DAYS + ANOMALY_INJECTION_DAYS\n",
    "\n",
    "print(f\"\\n--- 4. Generating Historical Data ({HISTORICAL_DAYS} days of normal behavior) ---\")\n",
    "\n",
    "# Start from 21 days ago, rounded to the hour for consistency\n",
    "current_time = datetime.now().replace(minute=0, second=0, microsecond=0)\n",
    "start_date = current_time - timedelta(days=TOTAL_DAYS)\n",
    "\n",
    "for i in range(HISTORICAL_DAYS):\n",
    "    hist_time = start_date + timedelta(days=i)\n",
    "    \n",
    "    batch_time = hist_time\n",
    "    anomaly_type = None # No anomalies in the first 14 days\n",
    "    num_records = 5000\n",
    "    \n",
    "    print(f\"   -> Generating historical batch for: {batch_time.strftime('%Y-%m-%d')} (Normal)\")\n",
    "\n",
    "    # Generate batch\n",
    "    v, app, acc, txn = generate_batch(batch_time, num_records=num_records, anomaly_type=anomaly_type)\n",
    "\n",
    "    write_data_to_landing_zone(v, LANDING_PATHS['visitor'], \"hist\", \"timestamp\")\n",
    "    write_data_to_landing_zone(app, LANDING_PATHS['app'], \"hist\", \"timestamp\")\n",
    "    write_data_to_landing_zone(acc, LANDING_PATHS['account'], \"hist\", \"account_open_timestamp\")\n",
    "    write_data_to_landing_zone(txn, LANDING_PATHS['txn'], \"hist\", \"timestamp\")\n",
    "\n",
    "\n",
    "# --- 5. EXECUTION: STREAMING DATA GENERATION with INJECTED ANOMALIES (7 DAYS) ---\n",
    "\n",
    "print(f\"\\n--- 5. Generating Live Data with Injected Anomalies ({ANOMALY_INJECTION_DAYS} days) ---\")\n",
    "\n",
    "# Define a robust daily schedule of anomalies for the last 7 days\n",
    "DAILY_ANOMALY_SCHEDULE = [\n",
    "    # (Anomaly Type, Base Records)\n",
    "    (None, 4500),         # Normal control volume\n",
    "    ('null_burst', 5000), # TDQ Nulls\n",
    "    ('age_error', 5500),  # BDQ Rule Violation (Age < 18)\n",
    "    (None, 4800),         # Normal control volume\n",
    "    ('traffic_spike', 2000), # ML Bot Traffic (low records, high duration/low conversion)\n",
    "    ('fraud_spike', 5200),# ML Outlier\n",
    "    ('limit_spike', 5300),# ML Outlier (Suspiciously high limit request)\n",
    "    ('stale_data', 4800), #F RESHNESS ANOMALY\n",
    "    (None, 4900),         # Normal control volume\n",
    "]\n",
    "\n",
    "# Loop through the last 7 days\n",
    "for i in range(ANOMALY_INJECTION_DAYS):\n",
    "    current_date = start_date + timedelta(days=HISTORICAL_DAYS + i)\n",
    "    print(f\"\\n   -> Injecting anomalies for date: {current_date.strftime('%Y-%m-%d')}\")\n",
    "    \n",
    "    # Run 8 micro-batches spread throughout the day\n",
    "    for j, (anomaly_type, base_records) in enumerate(DAILY_ANOMALY_SCHEDULE):\n",
    "        # Offset the time by 3 hours for each micro-batch (8 batches * 3 hours = 24 hours)\n",
    "        batch_time = current_date + timedelta(hours=j * 3)\n",
    "        desc = anomaly_type if anomaly_type else \"Normal\"\n",
    "        \n",
    "        print(f\"      -> Batch {j+1}: {desc} at {batch_time.strftime('%H:%M:%S')}\")\n",
    "        \n",
    "        # Generate batch with anomaly flag\n",
    "        v, app, acc, txn = generate_batch(batch_time, num_records=base_records, anomaly_type=anomaly_type)\n",
    "        \n",
    "        # Write files with a unique timestamp prefix\n",
    "        prefix = batch_time.strftime(\"%Y%m%d%H%M%S\")\n",
    "        write_data_to_landing_zone(v, LANDING_PATHS['visitor'], f\"{prefix}_live\", \"timestamp\")\n",
    "        write_data_to_landing_zone(app, LANDING_PATHS['app'], f\"{prefix}_live\", \"timestamp\")\n",
    "        write_data_to_landing_zone(acc, LANDING_PATHS['account'], f\"{prefix}_live\", \"account_open_timestamp\")\n",
    "        write_data_to_landing_zone(txn, LANDING_PATHS['txn'], f\"{prefix}_live\", \"timestamp\")\n",
    "\n",
    "\n",
    "print(\"\\nData Generation Complete.\")\n",
    "print(f\"Source files are staged in: {BASE_PATH_UC}landing/\")\n",
    "\n",
    "# --- 6. VERIFICATION ---\n",
    "# Verify that the files are ready for streaming ingestion\n",
    "print(\"\\n--- 6. VERIFICATION (Checking raw data counts) ---\")\n",
    "try:\n",
    "    total_visitors = spark.read.parquet(LANDING_PATHS['visitor']).count()\n",
    "    total_applications = spark.read.parquet(LANDING_PATHS['app']).count()\n",
    "    print(f\"Total Visitor Records Generated: {total_visitors:,}\")\n",
    "    print(f\"Total Application Records Generated: {total_applications:,}\")\n",
    "except Exception as e:\n",
    "    print(f\"Verification Error: Could not read or list files. Error: {e}\")\n",
    "\n",
    "\n",
    "# Store the base path for use in the next notebook\n",
    "dbutils.notebook.exit(BASE_PATH_UC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e62657f2-93f2-425c-a7df-32c76813d6c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "387e28c3-cb8a-4705-8656-a516ee1e9f3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8844922580010058,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Data_Simulation_and_Ingestion",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}